{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e762248a-4aa4-4cc9-b73e-2d24817e441d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:17.090018Z",
     "iopub.status.busy": "2023-06-10T20:03:17.089196Z",
     "iopub.status.idle": "2023-06-10T20:03:17.094685Z",
     "shell.execute_reply": "2023-06-10T20:03:17.093913Z",
     "shell.execute_reply.started": "2023-06-10T20:03:17.089969Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1506e-d0ea-4a66-8c3c-5febe458cfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f53c99-ec37-4e46-9d7b-e5ea1ed8e8ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:17.096897Z",
     "iopub.status.busy": "2023-06-10T20:03:17.096568Z",
     "iopub.status.idle": "2023-06-10T20:03:17.104917Z",
     "shell.execute_reply": "2023-06-10T20:03:17.104366Z",
     "shell.execute_reply.started": "2023-06-10T20:03:17.096869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64786344-7654-40ff-9253-f0c10c18d144",
   "metadata": {},
   "source": [
    "Módulos a importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd1521d-1809-4e1a-8957-6824806ab2b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:17.106378Z",
     "iopub.status.busy": "2023-06-10T20:03:17.105739Z",
     "iopub.status.idle": "2023-06-10T20:03:17.633372Z",
     "shell.execute_reply": "2023-06-10T20:03:17.632774Z",
     "shell.execute_reply.started": "2023-06-10T20:03:17.106358Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a7dcf7-e91e-40b4-a30b-70a428bb12ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:17.634745Z",
     "iopub.status.busy": "2023-06-10T20:03:17.634314Z",
     "iopub.status.idle": "2023-06-10T20:03:17.678476Z",
     "shell.execute_reply": "2023-06-10T20:03:17.677879Z",
     "shell.execute_reply.started": "2023-06-10T20:03:17.634711Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cfb250-c3df-40ae-988d-179466d65cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:17.679298Z",
     "iopub.status.busy": "2023-06-10T20:03:17.679085Z",
     "iopub.status.idle": "2023-06-10T20:03:20.633217Z",
     "shell.execute_reply": "2023-06-10T20:03:20.632588Z",
     "shell.execute_reply.started": "2023-06-10T20:03:17.679283Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/10 17:03:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Crea una configuración de Spark\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"8g\")  # Ajusta la memoria del executor\n",
    "conf.set(\"spark.driver.memory\", \"4g\")  # Ajusta la memoria del driver\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "768694c2-998a-47c4-afdd-949600adbc77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T17:02:31.705495Z",
     "iopub.status.busy": "2023-06-05T17:02:31.705267Z",
     "iopub.status.idle": "2023-06-05T17:02:31.994477Z",
     "shell.execute_reply": "2023-06-05T17:02:31.980692Z",
     "shell.execute_reply.started": "2023-06-05T17:02:31.705475Z"
    }
   },
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().set(\"spark.executor.memory\", \"4g\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772f2fa-2256-4368-8633-6b012945081a",
   "metadata": {},
   "source": [
    "Los códigos de estado HTTP son números de tres dígitos que se utilizan para indicar el resultado de una solicitud al servidor. Algunos ejemplos comunes de códigos de estado son:\n",
    "\n",
    "    200 OK: La solicitud ha sido exitosa.\n",
    "    400 Bad Request: La solicitud no se pudo entender o contiene sintaxis incorrecta.\n",
    "    401 Unauthorized: La solicitud requiere autenticación o el usuario no tiene los permisos necesarios.\n",
    "    404 Not Found: El recurso solicitado no se encuentra en el servidor.\n",
    "    500 Internal Server Error: El servidor encontró una condición inesperada que le impidió cumplir con la solicitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb03bc8-eab8-4a2b-bb73-e5fc5540b1f0",
   "metadata": {},
   "source": [
    "# Esperanza de vida al nacer, total (años)\n",
    "\n",
    "La esperanza de vida al nacer indica la cantidad de años que viviría un recién nacido si los patrones de mortalidad vigentes al momento de su nacimiento no cambian a lo largo de la vida del infante.\n",
    "\n",
    "https://datos.bancomundial.org/indicador/SP.DYN.LE00.IN?view=chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89288d7d-39b9-4fbf-ad8a-ecd080ac1fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:20.634494Z",
     "iopub.status.busy": "2023-06-10T20:03:20.634160Z",
     "iopub.status.idle": "2023-06-10T20:03:20.638779Z",
     "shell.execute_reply": "2023-06-10T20:03:20.638103Z",
     "shell.execute_reply.started": "2023-06-10T20:03:20.634448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_00 = \"https://api.worldbank.org/v2/es/indicator/SP.DYN.LE00.IN?downloadformat=csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663ab509-e1be-4917-ac22-3975c4d5609a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:20.641563Z",
     "iopub.status.busy": "2023-06-10T20:03:20.641140Z",
     "iopub.status.idle": "2023-06-10T20:03:22.007814Z",
     "shell.execute_reply": "2023-06-10T20:03:22.005155Z",
     "shell.execute_reply.started": "2023-06-10T20:03:20.641545Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_url_00 = requests.get(url_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefc5ff6-0789-4370-80d1-42a1aab38d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:22.013318Z",
     "iopub.status.busy": "2023-06-10T20:03:22.012437Z",
     "iopub.status.idle": "2023-06-10T20:03:22.045876Z",
     "shell.execute_reply": "2023-06-10T20:03:22.043966Z",
     "shell.execute_reply.started": "2023-06-10T20:03:22.013247Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo ZIP se ha descargado y descomprimido correctamente.\n"
     ]
    }
   ],
   "source": [
    "if response_url_00.status_code == 200:\n",
    "    # Crear un objeto ZipFile a partir de los datos de la respuesta\n",
    "    with zipfile.ZipFile(io.BytesIO(response_url_00.content), \"r\") as zip_url_00:\n",
    "        # Extraer todos los archivos del archivo ZIP en el directorio actual\n",
    "        zip_url_00.extractall(\"dataset\")\n",
    "\n",
    "    print(\"El archivo ZIP se ha descargado y descomprimido correctamente.\")\n",
    "else:\n",
    "    print(\"Error al descargar el archivo ZIP:\", response_url_00.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933e642f-6b87-4080-b6d4-a3dcc92e47b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:22.048314Z",
     "iopub.status.busy": "2023-06-10T20:03:22.047833Z",
     "iopub.status.idle": "2023-06-10T20:03:22.054374Z",
     "shell.execute_reply": "2023-06-10T20:03:22.053071Z",
     "shell.execute_reply.started": "2023-06-10T20:03:22.048276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ruta del archivo CSV\n",
    "\n",
    "csv_url_00 = \"dataset/API_SP.DYN.LE00.IN_DS2_es_csv_v2_5466462.csv\"\n",
    "\n",
    "# Crear una lista para almacenar las filas del archivo CSV\n",
    "rows_url_00 = []\n",
    "\n",
    "\n",
    "#API_SE.PRM.CMPT.ZS_DS2_es_csv_v2_5478942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56500448-a26f-42b3-b95e-031726a0ebc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:22.057045Z",
     "iopub.status.busy": "2023-06-10T20:03:22.056307Z",
     "iopub.status.idle": "2023-06-10T20:03:22.068370Z",
     "shell.execute_reply": "2023-06-10T20:03:22.067547Z",
     "shell.execute_reply.started": "2023-06-10T20:03:22.057006Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Leer el archivo CSV y almacenar las filas en la lista\n",
    "with open(csv_url_00, \"r\") as file_csv_url_00:\n",
    "    csv_reader_url_00 = csv.reader(file_csv_url_00)\n",
    "    rows_url_00 = list(csv_reader_url_00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53576dc9-a1c9-41e2-b2c1-27b362861348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:22.069725Z",
     "iopub.status.busy": "2023-06-10T20:03:22.069375Z",
     "iopub.status.idle": "2023-06-10T20:03:22.080946Z",
     "shell.execute_reply": "2023-06-10T20:03:22.080262Z",
     "shell.execute_reply.started": "2023-06-10T20:03:22.069693Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eliminar las primeras 4 filas\n",
    "\n",
    "rows_url_00 = rows_url_00[3:]\n",
    "\n",
    "# Escribir las filas modificadas en un nuevo archivo CSV\n",
    "with open(\"dataset/data/data_00.csv\", \"w\", newline=\"\") as file_csv_url_00:\n",
    "    csv_writer_url_00 = csv.writer(file_csv_url_00)\n",
    "    csv_writer_url_00.writerows(rows_url_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6512ad8f-3a37-49c8-94bd-895e334ae742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:22.082214Z",
     "iopub.status.busy": "2023-06-10T20:03:22.081874Z",
     "iopub.status.idle": "2023-06-10T20:03:25.029657Z",
     "shell.execute_reply": "2023-06-10T20:03:25.029121Z",
     "shell.execute_reply.started": "2023-06-10T20:03:22.082192Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_00 = spark.read.csv(\"dataset/data/data_00.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732d004e-a918-41a1-9fc2-33b98bfb7f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:25.030423Z",
     "iopub.status.busy": "2023-06-10T20:03:25.030260Z",
     "iopub.status.idle": "2023-06-10T20:03:25.062205Z",
     "shell.execute_reply": "2023-06-10T20:03:25.061649Z",
     "shell.execute_reply.started": "2023-06-10T20:03:25.030408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_00 = data_00.drop(\"_c66\",\"_c67\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20da272d-077f-4ebd-b2d7-b5875bbd512e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:25.063155Z",
     "iopub.status.busy": "2023-06-10T20:03:25.062830Z",
     "iopub.status.idle": "2023-06-10T20:03:26.360685Z",
     "shell.execute_reply": "2023-06-10T20:03:26.360232Z",
     "shell.execute_reply.started": "2023-06-10T20:03:25.063139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "23/06/10 17:03:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1961</th>\n",
       "      <th>1962</th>\n",
       "      <th>1963</th>\n",
       "      <th>1964</th>\n",
       "      <th>1965</th>\n",
       "      <th>1966</th>\n",
       "      <th>1967</th>\n",
       "      <th>1968</th>\n",
       "      <th>1969</th>\n",
       "      <th>1970</th>\n",
       "      <th>1971</th>\n",
       "      <th>1972</th>\n",
       "      <th>1973</th>\n",
       "      <th>1974</th>\n",
       "      <th>1975</th>\n",
       "      <th>1976</th>\n",
       "      <th>1977</th>\n",
       "      <th>1978</th>\n",
       "      <th>1979</th>\n",
       "      <th>1980</th>\n",
       "      <th>1981</th>\n",
       "      <th>1982</th>\n",
       "      <th>1983</th>\n",
       "      <th>1984</th>\n",
       "      <th>1985</th>\n",
       "      <th>1986</th>\n",
       "      <th>1987</th>\n",
       "      <th>1988</th>\n",
       "      <th>1989</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Esperanza de vida al nacer, total (años)</td>\n",
       "      <td>SP.DYN.LE00.IN</td>\n",
       "      <td>64.152</td>\n",
       "      <td>64.537</td>\n",
       "      <td>64.752</td>\n",
       "      <td>65.132</td>\n",
       "      <td>65.294</td>\n",
       "      <td>65.502</td>\n",
       "      <td>66.063</td>\n",
       "      <td>66.439</td>\n",
       "      <td>66.757</td>\n",
       "      <td>67.168</td>\n",
       "      <td>67.583</td>\n",
       "      <td>67.975</td>\n",
       "      <td>68.577</td>\n",
       "      <td>69.092</td>\n",
       "      <td>69.503</td>\n",
       "      <td>69.762</td>\n",
       "      <td>70.035</td>\n",
       "      <td>70.264</td>\n",
       "      <td>70.494</td>\n",
       "      <td>70.778</td>\n",
       "      <td>71.066</td>\n",
       "      <td>71.722</td>\n",
       "      <td>71.959</td>\n",
       "      <td>72.105</td>\n",
       "      <td>72.251</td>\n",
       "      <td>72.388</td>\n",
       "      <td>72.462</td>\n",
       "      <td>72.789</td>\n",
       "      <td>73.047</td>\n",
       "      <td>73.023</td>\n",
       "      <td>73.076</td>\n",
       "      <td>73.1</td>\n",
       "      <td>73.179</td>\n",
       "      <td>73.225</td>\n",
       "      <td>73.272</td>\n",
       "      <td>73.349</td>\n",
       "      <td>73.448</td>\n",
       "      <td>73.452</td>\n",
       "      <td>73.491</td>\n",
       "      <td>73.561</td>\n",
       "      <td>73.569</td>\n",
       "      <td>73.647</td>\n",
       "      <td>73.726</td>\n",
       "      <td>73.752</td>\n",
       "      <td>73.576</td>\n",
       "      <td>73.811</td>\n",
       "      <td>74.026</td>\n",
       "      <td>74.21</td>\n",
       "      <td>74.147</td>\n",
       "      <td>74.56</td>\n",
       "      <td>75.404</td>\n",
       "      <td>75.465</td>\n",
       "      <td>75.531</td>\n",
       "      <td>75.636</td>\n",
       "      <td>75.601</td>\n",
       "      <td>75.683</td>\n",
       "      <td>75.617</td>\n",
       "      <td>75.903</td>\n",
       "      <td>76.072</td>\n",
       "      <td>76.248</td>\n",
       "      <td>75.723</td>\n",
       "      <td>74.626</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Esperanza de vida al nacer, total (años)</td>\n",
       "      <td>SP.DYN.LE00.IN</td>\n",
       "      <td>44.0855518454965</td>\n",
       "      <td>44.386696928982</td>\n",
       "      <td>44.7521818697994</td>\n",
       "      <td>44.9131593023895</td>\n",
       "      <td>45.4790430350925</td>\n",
       "      <td>45.4983377851245</td>\n",
       "      <td>45.249104629175</td>\n",
       "      <td>45.9249050028634</td>\n",
       "      <td>46.2230965735857</td>\n",
       "      <td>46.4323030920124</td>\n",
       "      <td>46.7184845197579</td>\n",
       "      <td>47.1929422628056</td>\n",
       "      <td>46.8973892484582</td>\n",
       "      <td>47.6923174333564</td>\n",
       "      <td>47.5980637543936</td>\n",
       "      <td>47.7598869102852</td>\n",
       "      <td>48.3495880515137</td>\n",
       "      <td>48.6359051941044</td>\n",
       "      <td>48.7636045995518</td>\n",
       "      <td>49.261335737194</td>\n",
       "      <td>49.6365376633939</td>\n",
       "      <td>50.0570732913929</td>\n",
       "      <td>50.2968491379812</td>\n",
       "      <td>48.7033305923076</td>\n",
       "      <td>48.6526645552392</td>\n",
       "      <td>49.0116307202704</td>\n",
       "      <td>49.6397190079537</td>\n",
       "      <td>50.0758875408274</td>\n",
       "      <td>49.3597265265243</td>\n",
       "      <td>50.6840997852884</td>\n",
       "      <td>50.6077275337211</td>\n",
       "      <td>50.3904596093263</td>\n",
       "      <td>49.9621136475362</td>\n",
       "      <td>50.2736279712378</td>\n",
       "      <td>50.882581820857</td>\n",
       "      <td>51.0019252899341</td>\n",
       "      <td>50.8106889705148</td>\n",
       "      <td>50.9742313589474</td>\n",
       "      <td>50.3259127442221</td>\n",
       "      <td>51.2378519083819</td>\n",
       "      <td>51.9644813247726</td>\n",
       "      <td>52.189648214333</td>\n",
       "      <td>52.540794004192</td>\n",
       "      <td>53.0220345898659</td>\n",
       "      <td>53.5454571098034</td>\n",
       "      <td>54.2196508058582</td>\n",
       "      <td>55.1505461100496</td>\n",
       "      <td>55.9338017149142</td>\n",
       "      <td>56.6804188973962</td>\n",
       "      <td>57.6208515442432</td>\n",
       "      <td>58.4111497501441</td>\n",
       "      <td>59.2932713889337</td>\n",
       "      <td>60.0507802468673</td>\n",
       "      <td>60.709863140516</td>\n",
       "      <td>61.337911447657</td>\n",
       "      <td>61.8564524336049</td>\n",
       "      <td>62.4440448894169</td>\n",
       "      <td>62.9223846035263</td>\n",
       "      <td>63.3658579512383</td>\n",
       "      <td>63.7556736140197</td>\n",
       "      <td>63.3138559861117</td>\n",
       "      <td>62.4545852699516</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afganistán</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Esperanza de vida al nacer, total (años)</td>\n",
       "      <td>SP.DYN.LE00.IN</td>\n",
       "      <td>32.535</td>\n",
       "      <td>33.068</td>\n",
       "      <td>33.547</td>\n",
       "      <td>34.016</td>\n",
       "      <td>34.494</td>\n",
       "      <td>34.953</td>\n",
       "      <td>35.453</td>\n",
       "      <td>35.924</td>\n",
       "      <td>36.418</td>\n",
       "      <td>36.91</td>\n",
       "      <td>37.418</td>\n",
       "      <td>37.923</td>\n",
       "      <td>38.444</td>\n",
       "      <td>39.003</td>\n",
       "      <td>39.55</td>\n",
       "      <td>40.1</td>\n",
       "      <td>40.645</td>\n",
       "      <td>41.228</td>\n",
       "      <td>40.271</td>\n",
       "      <td>39.086</td>\n",
       "      <td>39.618</td>\n",
       "      <td>40.164</td>\n",
       "      <td>37.766</td>\n",
       "      <td>38.187</td>\n",
       "      <td>33.329</td>\n",
       "      <td>33.55</td>\n",
       "      <td>39.396</td>\n",
       "      <td>39.844</td>\n",
       "      <td>43.958</td>\n",
       "      <td>45.158</td>\n",
       "      <td>45.967</td>\n",
       "      <td>46.663</td>\n",
       "      <td>47.596</td>\n",
       "      <td>51.466</td>\n",
       "      <td>51.495</td>\n",
       "      <td>52.544</td>\n",
       "      <td>53.243</td>\n",
       "      <td>53.634</td>\n",
       "      <td>52.943</td>\n",
       "      <td>54.846</td>\n",
       "      <td>55.298</td>\n",
       "      <td>55.798</td>\n",
       "      <td>56.454</td>\n",
       "      <td>57.344</td>\n",
       "      <td>57.944</td>\n",
       "      <td>58.361</td>\n",
       "      <td>58.684</td>\n",
       "      <td>59.111</td>\n",
       "      <td>59.852</td>\n",
       "      <td>60.364</td>\n",
       "      <td>60.851</td>\n",
       "      <td>61.419</td>\n",
       "      <td>61.923</td>\n",
       "      <td>62.417</td>\n",
       "      <td>62.545</td>\n",
       "      <td>62.659</td>\n",
       "      <td>63.136</td>\n",
       "      <td>63.016</td>\n",
       "      <td>63.081</td>\n",
       "      <td>63.565</td>\n",
       "      <td>62.575</td>\n",
       "      <td>61.982</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country Name Country Code                            Indicator Name  Indicator Code              1960             1961              1962              1963              1964              1965             1966              1967              1968              1969              1970              1971              1972              1973              1974              1975              1976              1977              1978             1979              1980              1981              1982              1983              1984              1985              1986              1987              1988              1989              1990              1991              1992              1993             1994              1995              1996              1997              1998              1999              2000             2001             2002              2003              2004              2005              2006              2007              2008              2009              2010              2011              2012             2013             2014              2015              2016              2017              2018              2019              2020              2021  2022\n",
       "0        Aruba          ABW  Esperanza de vida al nacer, total (años)  SP.DYN.LE00.IN            64.152           64.537            64.752            65.132            65.294            65.502           66.063            66.439            66.757            67.168            67.583            67.975            68.577            69.092            69.503            69.762            70.035            70.264            70.494           70.778            71.066            71.722            71.959            72.105            72.251            72.388            72.462            72.789            73.047            73.023            73.076              73.1            73.179            73.225           73.272            73.349            73.448            73.452            73.491            73.561            73.569           73.647           73.726            73.752            73.576            73.811            74.026             74.21            74.147             74.56            75.404            75.465            75.531           75.636           75.601            75.683            75.617            75.903            76.072            76.248            75.723            74.626  None\n",
       "1         None          AFE  Esperanza de vida al nacer, total (años)  SP.DYN.LE00.IN  44.0855518454965  44.386696928982  44.7521818697994  44.9131593023895  45.4790430350925  45.4983377851245  45.249104629175  45.9249050028634  46.2230965735857  46.4323030920124  46.7184845197579  47.1929422628056  46.8973892484582  47.6923174333564  47.5980637543936  47.7598869102852  48.3495880515137  48.6359051941044  48.7636045995518  49.261335737194  49.6365376633939  50.0570732913929  50.2968491379812  48.7033305923076  48.6526645552392  49.0116307202704  49.6397190079537  50.0758875408274  49.3597265265243  50.6840997852884  50.6077275337211  50.3904596093263  49.9621136475362  50.2736279712378  50.882581820857  51.0019252899341  50.8106889705148  50.9742313589474  50.3259127442221  51.2378519083819  51.9644813247726  52.189648214333  52.540794004192  53.0220345898659  53.5454571098034  54.2196508058582  55.1505461100496  55.9338017149142  56.6804188973962  57.6208515442432  58.4111497501441  59.2932713889337  60.0507802468673  60.709863140516  61.337911447657  61.8564524336049  62.4440448894169  62.9223846035263  63.3658579512383  63.7556736140197  63.3138559861117  62.4545852699516  None\n",
       "2   Afganistán          AFG  Esperanza de vida al nacer, total (años)  SP.DYN.LE00.IN            32.535           33.068            33.547            34.016            34.494            34.953           35.453            35.924            36.418             36.91            37.418            37.923            38.444            39.003             39.55              40.1            40.645            41.228            40.271           39.086            39.618            40.164            37.766            38.187            33.329             33.55            39.396            39.844            43.958            45.158            45.967            46.663            47.596            51.466           51.495            52.544            53.243            53.634            52.943            54.846            55.298           55.798           56.454            57.344            57.944            58.361            58.684            59.111            59.852            60.364            60.851            61.419            61.923           62.417           62.545            62.659            63.136            63.016            63.081            63.565            62.575            61.982  None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_00.limit(3).pandas_api()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8caedda6-d947-4ba1-adb2-e1d32afe03a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:26.361538Z",
     "iopub.status.busy": "2023-06-10T20:03:26.361364Z",
     "iopub.status.idle": "2023-06-10T20:03:26.459898Z",
     "shell.execute_reply": "2023-06-10T20:03:26.459202Z",
     "shell.execute_reply.started": "2023-06-10T20:03:26.361522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_00 = data_00.selectExpr('`Country Name`', '`Country Code`', \"`Indicator Name`\", \"`Indicator Code`\", \"stack(63, \\\n",
    "'1960', `1960`, '1961', `1961`, '1962', `1962`, '1963', `1963`, '1964', `1964`, \\\n",
    "'1965', `1965`, '1966', `1966`, '1967', `1967`, '1968', `1968`, '1969', `1969`, \\\n",
    "'1970', `1970`, '1971', `1971`, '1972', `1972`, '1973', `1973`, '1974', `1974`, \\\n",
    "'1975', `1975`, '1976', `1976`, '1977', `1977`, '1978', `1978`, '1979', `1979`, \\\n",
    "'1980', `1980`, '1981', `1981`, '1982', `1982`, '1983', `1983`, '1984', `1984`, \\\n",
    "'1985', `1985`, '1986', `1986`, '1987', `1987`, '1988', `1988`, '1989', `1989`, \\\n",
    "'1990', `1990`, '1991', `1991`, '1992', `1992`, '1993', `1993`, '1994', `1994`, \\\n",
    "'1995', `1995`, '1996', `1996`, '1997', `1997`, '1998', `1998`, '1999', `1999`, \\\n",
    "'2000', `2000`, '2001', `2001`, '2002', `2002`, '2003', `2003`, '2004', `2004`, \\\n",
    "'2005', `2005`, '2006', `2006`, '2007', `2007`, '2008', `2008`, '2009', `2009`, \\\n",
    "'2010', `2010`, '2011', `2011`, '2012', `2012`, '2013', `2013`, '2014', `2014`, \\\n",
    "'2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`, '2019', `2019`, \\\n",
    "'2020', `2020`, '2021', `2021`, '2022', `2022`) as (Year, life_expatancy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35dad817-0450-4e52-8d9c-57dccd7ece11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T20:03:26.461016Z",
     "iopub.status.busy": "2023-06-10T20:03:26.460772Z",
     "iopub.status.idle": "2023-06-10T20:03:27.382938Z",
     "shell.execute_reply": "2023-06-10T20:03:27.381886Z",
     "shell.execute_reply.started": "2023-06-10T20:03:26.460992Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/10 17:03:26 ERROR FileOutputCommitter: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0\n",
      "23/06/10 17:03:26 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 3)\n",
      "java.io.IOException: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0/_temporary/attempt_20230610170326942933577633855912_0004_m_000000_3 (exists=false, cwd=file:/_disk_dev/final_project)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "23/06/10 17:03:26 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 3) (pablo executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0/_temporary/attempt_20230610170326942933577633855912_0004_m_000000_3 (exists=false, cwd=file:/_disk_dev/final_project)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "\n",
      "23/06/10 17:03:26 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job\n",
      "23/06/10 17:03:26 ERROR FileFormatWriter: Aborting job caeb58ef-e889-4391-9298-7b86d9944777.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3) (pablo executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0/_temporary/attempt_20230610170326942933577633855912_0004_m_000000_3 (exists=false, cwd=file:/_disk_dev/final_project)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0/_temporary/attempt_20230610170326942933577633855912_0004_m_000000_3 (exists=false, cwd=file:/_disk_dev/final_project)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o366.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3) (pablo executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0/_temporary/attempt_20230610170326942933577633855912_0004_m_000000_3 (exists=false, cwd=file:/_disk_dev/final_project)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\nCaused by: java.io.IOException: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0/_temporary/attempt_20230610170326942933577633855912_0004_m_000000_3 (exists=false, cwd=file:/_disk_dev/final_project)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_00\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1782\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1783\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1797\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1798\u001b[0m )\n\u001b[0;32m-> 1799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o366.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 3) (pablo executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0/_temporary/attempt_20230610170326942933577633855912_0004_m_000000_3 (exists=false, cwd=file:/_disk_dev/final_project)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\nCaused by: java.io.IOException: Mkdirs failed to create file:/content/drive/MyDrive/data/_temporary/0/_temporary/attempt_20230610170326942933577633855912_0004_m_000000_3 (exists=false, cwd=file:/_disk_dev/final_project)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "data_00.coalesce(1).write.option(\"header\", \"true\").csv(\"/content/drive/MyDrive/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9c552-3a4c-4b82-8c08-fed2e56eee5b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-10T20:03:27.383412Z",
     "iopub.status.idle": "2023-06-10T20:03:27.383636Z",
     "shell.execute_reply": "2023-06-10T20:03:27.383535Z",
     "shell.execute_reply.started": "2023-06-10T20:03:27.383523Z"
    }
   },
   "outputs": [],
   "source": [
    "/content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a865cb-59f1-4e44-888f-b959b2f0163a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-10T20:03:27.384847Z",
     "iopub.status.idle": "2023-06-10T20:03:27.385336Z",
     "shell.execute_reply": "2023-06-10T20:03:27.385198Z",
     "shell.execute_reply.started": "2023-06-10T20:03:27.385183Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcf0a9-a777-4fcb-8dc0-56848521628a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-10T20:03:27.386203Z",
     "iopub.status.idle": "2023-06-10T20:03:27.386439Z",
     "shell.execute_reply": "2023-06-10T20:03:27.386345Z",
     "shell.execute_reply.started": "2023-06-10T20:03:27.386335Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Ruta al archivo JSON de las credenciales descargado anteriormente\n",
    "credentials_file = 'credential.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e5d0e-e7bf-4b42-a82a-6ac72cfd4ff0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-10T20:03:27.387257Z",
     "iopub.status.idle": "2023-06-10T20:03:27.387684Z",
     "shell.execute_reply": "2023-06-10T20:03:27.387543Z",
     "shell.execute_reply.started": "2023-06-10T20:03:27.387528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crear una instancia del servicio de Google Drive\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    credentials_file,\n",
    "    scopes=['https://www.googleapis.com/auth/drive']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e5d27-bb1b-4cc0-8156-a1360998aeeb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-10T20:03:27.388785Z",
     "iopub.status.idle": "2023-06-10T20:03:27.389167Z",
     "shell.execute_reply": "2023-06-10T20:03:27.389069Z",
     "shell.execute_reply.started": "2023-06-10T20:03:27.389058Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drive_service = build('drive', 'v3', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916b22e-1705-4d84-a107-0d7adf206929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba64a3-3a0d-40d9-9b4f-99dd6fafcfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5c6a3-f621-44b1-a164-616cc48b6f0c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-10T20:03:27.389989Z",
     "iopub.status.idle": "2023-06-10T20:03:27.390258Z",
     "shell.execute_reply": "2023-06-10T20:03:27.390160Z",
     "shell.execute_reply.started": "2023-06-10T20:03:27.390149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "# Crea una instancia de GoogleAuth\n",
    "gauth = GoogleAuth()\n",
    "\n",
    "# Inicia sesión en Google Drive utilizando las credenciales\n",
    "gauth.LocalWebserverAuth(port_numbers=[8081])\n",
    "\n",
    "# Crea una instancia de GoogleDrive utilizando la autenticación\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba687c6-07fb-4ec8-ba61-411263cefb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
